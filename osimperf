#!/usr/bin/env python3

# OpenSim performance benchmarking runner
#
# This python script implements a command-line interface for running
# OpenSim performance benchmarks. The main utility of this is to
# automate and standardize the performance benchmarks so that it's
# easy to aggregate statistics, add new metrics, etc. (rather than
# having a manual approach, or a bunch of seperate scripts).

import argparse
import tempfile
import glob
import shutil
import errno
import os
import subprocess
import re
import sys
import time

# Represents a single test case in the suite. This benchmarking suite
# contains a few tests that exercise different parts of OpenSim.
class TestCase:
    def __init__(self, *, name, description="", url="", files, setup_file, libraries=[]):
        self.name = name
        self.description = description
        self.url = url
        self.files = files
        self.setup_file = setup_file
        self.libraries = libraries

# List of test cases being exercised by this suite
test_cases = [
    TestCase(
        name="Gait2354",
        description="IK simulation from OpenSim tutorial",
        url="https://simtk-confluence.stanford.edu/display/OpenSim/Tutorial+3+-+Scaling%2C+Inverse+Kinematics%2C+and+Inverse+Dynamics",
        files=["Gait2354_Simbody/*"],
        setup_file="subject01_Setup_InverseDynamics.xml"),
    TestCase(
        name="ToyDropLanding",
        description="FD simulation from OpenSim tutorial",
        url="https://simtk-confluence.stanford.edu/display/OpenSim/Simulation-Based+Design+to+Prevent+Ankle+Injuries",
        files=["ToyDropLanding/*"],
        setup_file="setup_forward_tool.xml"),
    TestCase(
        name="ToyDropLanding_nomuscles",
        description="FD simulation from OpenSim tutorial (with <ForceSet /> removed)",
        url="https://simtk-confluence.stanford.edu/display/OpenSim/Simulation-Based+Design+to+Prevent+Ankle+Injuries",
        files=["ToyDropLanding_nomuscles/*"],
        setup_file="setup_forward_tool.xml"),
    TestCase(
        name="passive_dynamic",
        description="FD simulation from Ajay's collection",
        files=["passive_dynamic/*"],
        setup_file="setup_forward_tool.xml"),
    TestCase(
        name="passive_dynamic_noanalysis",
        description="FD simulation from Ajay's collection",
        files=["passive_dynamic_noanalysis/*"],
        setup_file="setup_forward_tool.xml"),
    TestCase(
        name="Arm26",
        description="Simulation of an arm bending slightly (/w muscle wrapping?)",
        files=["Arm26/*"],
        setup_file="arm26_Setup_Forward.xml"),
    TestCase(
        name="RajagopalModel",
        description="Model with muscle wrapping surfaces",
        files=["RajagopalModel/*"],
        setup_file="default_Setup_ForwardTool.xml"),
#    TestCase(
#        name="TestThoracoScapula",
#        description="From Ajay: TestThoracoScapula",
#        files=["TestThoracoScapula/*"],
#        setup_file="default_Setup_ForwardTool.xml",
#        libraries=["./libScapulothoracicJointPlugin40.so"]),
]

# The runtime context of a test. This class provides a test step with
# access to runtime values (e.g. scratch dirs, what options the user
# passed, etc.)
class TestContext:
    def __init__(self,
                 *,
                 scratch_dir,
                 skip_valgrind=False,
                 skip_perf_record=False,
                 outdir=None,
                 perf_record_callstack=False,
                 perf_stat_num_repeats=8,
                 opensim_cmd='opensim-cmd'):
        self.scratch_dir = scratch_dir
        self.skip_valgrind = skip_valgrind
        self.skip_perf_record = skip_perf_record
        self.outdir = outdir
        self.perf_record_callstack = perf_record_callstack
        self.perf_stat_num_repeats = perf_stat_num_repeats
        self.opensim_cmd = opensim_cmd

        if outdir and not os.path.exists(outdir):
            print(f"{outdir}: does not exist: creating")
            os.mkdir(outdir)

        if int(perf_stat_num_repeats) < 2:
            raise RuntimeError("number of repeats specified is too small. must be at least 2")

def setup_scratch_dir(ctx, test_case):
    for path in test_case.files:
        for entry in glob.glob(path):
            print(f"copying {entry} to {ctx.scratch_dir}")
            try:
                shutil.copytree(entry, os.path.join(ctx.scratch_dir, os.path.basename(entry)))
            except OSError as exc:
                if exc.errno == errno.ENOTDIR:
                    shutil.copy(entry, ctx.scratch_dir)
                else:
                    raise

def opensim_args(ctx, test_case):
    args = [ctx.opensim_cmd]
    for lib in test_case.libraries:
        args += ["-L", lib]
    args += ["run-tool", test_case.setup_file]

    return args

def run_application(*, args, **kwargs):
    return subprocess.run(args=args, **kwargs)

def run_valgrind(ctx, test_case):
    p = run_application(
        args=[
            "valgrind",
            "--tool=callgrind",
            "--dump-instr=yes",
            "--cache-sim=yes",
            "--branch-sim=yes",
            "--",
            *opensim_args(ctx, test_case),
        ],
        cwd=ctx.scratch_dir)
    assert p.returncode == 0

    if ctx.outdir:
        for outfile in glob.glob(os.path.join(ctx.scratch_dir, 'callgrind*')):
            print(f"writing valgrind file '{outfile}' to '{ctx.outdir}'")
            shutil.copy(outfile, ctx.outdir)

def run_perf(ctx, test_case):
    if ctx.perf_record_callstack:
        cg_arg = ["--call-graph", "dwarf"]
    else:
        cg_arg = []

    p = run_application(
        args=[
            "perf",
            "record",
            "--freq",
            "8192",
            *cg_arg,
            "--",
            *opensim_args(ctx, test_case),
        ],
        cwd=ctx.scratch_dir)
    assert p.returncode == 0

    if ctx.outdir:
        src = os.path.join(ctx.scratch_dir, 'perf.data')
        dest = ctx.outdir
        print(f"writing perf file '{src}' to '{dest}'")
        shutil.copy(src, dest)

def run_test_case(ctx, test_case):
    setup_scratch_dir(ctx, test_case)
    if not ctx.skip_valgrind:
        run_valgrind(ctx, test_case)
    if not ctx.skip_perf_record:
        run_perf(ctx, test_case)

# Stats collected from `perf` output
class Stats:
    def __init__(self, time_elapsed, time_elapsed_stderr, time_elapsed_pct_stderr):
        self.time_elapsed = time_elapsed
        self.time_elapsed_stderr = time_elapsed_stderr
        self.time_elapsed_pct_stderr = time_elapsed_pct_stderr

def stat_test_case(ctx, test_case):
    setup_scratch_dir(ctx, test_case)
    stat_path = os.path.join(ctx.scratch_dir, "perf.stats")
    durations = []
    for i in range(0, int(ctx.perf_stat_num_repeats)):
        start_time = time.time()
        p = run_application(
            args=opensim_args(ctx, test_case),
            cwd=ctx.scratch_dir)
        end_time = time.time()
        duration = end_time - start_time
        durations.append(duration)
    duration_avg = sum(durations)/len(durations)
    duration_stddev = sum(map(lambda dur: (dur - duration_avg)**2, durations))/(len(durations)-1)
    pct_err = duration_stddev/duration_avg
    print(f"{'%.6f' % duration_avg} +- {'%.6f' % duration_stddev} seconds time elapsed ( +- {'%.3f' % pct_err}% )")

    return Stats(
            time_elapsed=duration_avg,
            time_elapsed_stderr=duration_stddev,
            time_elapsed_pct_stderr=pct_err)

# Storage of a comparison between two stats
class StoredComparison:
    def __init__(self, test_case, stat_master, stat_branch):
        self.test_case = test_case
        self.stat_master = stat_master
        self.stat_branch = stat_branch

# Thin abstraction for collecting + printing output stats at runtime.
# This exists because I plan on having a `CliReporter` (interactive)
# and a `TableReporter` (collects inputs and aggregates into a nice
# table).
class Reporter:
    def __init__(self, num_repeats, lhs_opensim, rhs_opensim):
        print(f"lhs = {lhs_opensim}")
        print(f"rhs = {rhs_opensim}")
        print(f"repeats = {num_repeats}")
        self.comparisons = []

    def on_comparison(self, test_case, stat_master, stat_branch):
        sc = StoredComparison(test_case, stat_master, stat_branch)
        self.comparisons.append(sc)

    def on_finish(self):
        names = [sc.test_case for sc in self.comparisons]
        master_elapseds = [sc.stat_master.time_elapsed for sc in self.comparisons]
        master_stderrs = [sc.stat_master.time_elapsed_stderr for sc in self.comparisons]
        branch_elapseds = [sc.stat_branch.time_elapsed for sc in self.comparisons]
        branch_stderrs = [sc.stat_branch.time_elapsed_stderr for sc in self.comparisons]
        speedups = list(map(lambda p: p[0]/p[1], zip(master_elapseds, branch_elapseds)))

        to_2dp_str = lambda f: "%.2f" % f
        master_strs = [to_2dp_str(v) for v in master_elapseds]
        master_stderrs_strs = [to_2dp_str(v) for v in master_stderrs]
        branch_strs = [to_2dp_str(v) for v in branch_elapseds]
        branch_stderrs_strs = [to_2dp_str(v) for v in branch_stderrs]
        speedup_strs = [to_2dp_str(v) for v in speedups]

        cols = [
            names,
            master_strs,
            master_stderrs_strs,
            branch_strs,
            branch_stderrs_strs,
            speedup_strs,
        ]
        col_widths = [max(map(len, col)) for col in cols]

        headers = [
            "Test Name",
            "lhs [secs]",
            "σ [secs]",
            "rhs [secs]",
            "σ [secs]",
            "Speedup",
        ]

        # hack: fixup col widths for headers
        for i in range(0, len(col_widths)):
            col_widths[i] = max(col_widths[i], len(headers[i]))

        # print headers
        header_line = "|"
        for i in range(0, len(headers)):
            col_width = col_widths[i]
            header_line += ' '
            header_line += headers[i].rjust(col_width)
            header_line += ' |'
        print(header_line)

        # seperator
        separator_line = "|"
        for i in range(0, len(headers)):
            col_width = col_widths[i]
            separator_line += ' '
            separator_line += col_width * '-'
            separator_line += ' |'
        print(separator_line)

        # data rows
        for rownum in range(0, len(self.comparisons)):
            s = "|"
            for colnum in range(0, len(cols)):
                width = col_widths[colnum]
                data = cols[colnum][rownum]
                s += ' '
                s += data.rjust(width)
                s += ' |'
            print(s)

def get_testcase_by_name(test_name):
    for test_case in test_cases:
        if test_case.name == test_name:
            return test_case

    raise RuntimeError(f"{test_name}: no test case found: try 'ls' to list all available cases")

def cmd_ls(args):
    for test_case in test_cases:
        print(test_case.name)

def cmd_record(args):
    test_case = get_testcase_by_name(args.test_name)
    with tempfile.TemporaryDirectory() as tmpdir:
        ctx = TestContext(scratch_dir=tmpdir,
                          skip_valgrind=args.skip_valgrind,
                          skip_perf_record=args.skip_perf_record,
                          outdir=args.output,
                          perf_record_callstack=args.perf_record_callstack)
        run_test_case(ctx, test_case)

def cmd_stat(args):
    test_case = get_testcase_by_name(args.test_name)
    with tempfile.TemporaryDirectory() as tmpdir:
        ctx = TestContext(scratch_dir=tmpdir,
                          outdir=args.output,
                          perf_stat_num_repeats=args.repeats)
        stat_test_case(ctx, test_case)

def cmd_comparison(args):
    repeats = int(args.repeats)
    lhs_opensim = args.opensim_cmd_lhs
    rhs_opensim = args.opensim_cmd_rhs

    reporter = Reporter(num_repeats=repeats,
                        lhs_opensim=lhs_opensim,
                        rhs_opensim=rhs_opensim)

    for test_case in test_cases:
        with tempfile.TemporaryDirectory() as tmpdir:
            ctx = TestContext(scratch_dir=tmpdir,
                              perf_stat_num_repeats=repeats,
                              opensim_cmd=lhs_opensim)
            stats_master = stat_test_case(ctx, test_case)
        with tempfile.TemporaryDirectory() as tmpdir:
            ctx = TestContext(scratch_dir=tmpdir,
                              perf_stat_num_repeats=repeats,
                              opensim_cmd=rhs_opensim)
            stats_branch = stat_test_case(ctx, test_case)
        reporter.on_comparison(test_case.name, stats_master, stats_branch)
    reporter.on_finish()

def main():
    parser = argparse.ArgumentParser(
        description="OpenSim perf benchmark runner")
    subparsers = parser.add_subparsers(title="subcommands")

    run_sp = subparsers.add_parser("record", help="Record detailed perf trace of a test")
    run_sp.add_argument('test_name')
    run_sp.add_argument('-o', '--output', help="Output directory for recorded traces (default: temporary directory)", default=None)
    run_sp.add_argument('--skip-valgrind', help="Do not record valgrind trace", action='store_true')
    run_sp.add_argument('--skip-perf-record', help="Do not record with 'perf record'", action='store_true')
    run_sp.add_argument('--perf-record-callstack', help="Record callstack when running perf", action='store_true')
    run_sp.set_defaults(func=cmd_record)

    ls_sp = subparsers.add_parser("ls", help="List available tests")
    ls_sp.set_defaults(func=cmd_ls)

    measure_sp = subparsers.add_parser("stat", help="Measure top-level stats of a test")
    measure_sp.add_argument('test_name')
    measure_sp.add_argument('-o', '--output', help="Output directory for recorded traces (default: temporary directory)", default=None)
    measure_sp.add_argument('-r', '--repeats', help="Repeats per perf measurement", default=8)
    measure_sp.set_defaults(func=cmd_stat)

    comparison_sp = subparsers.add_parser("compare-all", help="compare two versions of opensim-cmd (defined using environment vars OPENSIM_MASTER_INSTALL and OPENSIM_BRANCH_INSTALL")
    comparison_sp.add_argument('opensim_cmd_lhs', help='opensim-cmd command to run (e.g. opensim-cmd, /some/path/to/opensim-cmd)')
    comparison_sp.add_argument('opensim_cmd_rhs', help='opensim-cmd command to run (e.g. opensim-cmd, /some/other/path/to/opensim-cmd)')
    comparison_sp.add_argument('-r', '--repeats', help="Repeats per comparison", default=8)
    comparison_sp.set_defaults(func=cmd_comparison)

    args = parser.parse_args()
    if hasattr(args, "func"):
        args.func(args)
        return 0
    else:
        parser.print_help(sys.stderr)
        return -1

if __name__ == "__main__":
    main()
