#!/usr/bin/env python3

# osimperf: performance benchmark runner script
#
# this script implements a top-level CLI for running OpenSim
# performance benchmarks. The main utility of this is to automate and
# standardize the performance benchmarks so that it's easy to
# aggregate statistics, add new metrics, etc. (rather than having a
# manual approach, or a bunch of seperate scripts).

import argparse
import tempfile
import glob
import shutil
import errno
import os
import subprocess
import re
import sys
import time
import configparser
import logging
import random

# global static log
logging.basicConfig(level=logging.WARNING)
log = logging.getLogger()


# initialized app state
class AppState:
    def __init__(self, test_cases):
        self.test_cases = test_cases

# a single test case in the suite
#
# for this high-level benchmark, a "test case" is essentially an
# OpenSim model (e.g. a muscular model of a human) + a tool
# configuration file (e.g. a forward dynamic simulation configuration
# file)
class TestCase:
    def __init__(self, *, name, description="", url="", files, setup_file, libraries=[]):
        self.name = name
        self.description = description
        self.url = url
        self.files = files
        self.setup_file = setup_file
        self.libraries = libraries

# loads a `TestCase` from an `osimperf.conf` file
def __load_testcase_from_config(path):
    log.debug("%s: loading test case", path)
    cfg = configparser.ConfigParser(interpolation=None)
    cfg.read(path)

    if cfg.has_section('files'):
        f = cfg['files']
        files = []
        for entry in f:
            files.append(f[entry])
    else:
        files = []

    g = cfg['general']

    return TestCase(
        name=g['name'],
        description=g.get('description', ''),
        url=g.get('url', ''),
        files=files,
        setup_file=g['setup_file'])

# loads all `osimperf.conf` files recursively found from current
# working dir
def __load_test_cases(d):
    patt = os.path.join(d, "**", "osimperf.conf")
    log.debug("loading all test cases via glob %s", patt)
    tcs = [
        __load_testcase_from_config(e)
        for e in glob.glob(patt)
    ]
    log.debug("loaded %s test cases", len(tcs))
    return tcs

# The runtime context of a particular test. This class provides a test
# step with access to runtime values (e.g. scratch dirs, what options
# the user passed, etc.)
class TestContext:
    def __init__(self,
                 *,
                 scratch_dir,
                 test_case,
                 skip_valgrind=False,
                 skip_perf_record=False,
                 outdir=None,
                 perf_record_callstack=False,
                 perf_stat_num_repeats=8,
                 opensim_cmd='opensim-cmd'):
        self.scratch_dir = scratch_dir
        self.skip_valgrind = skip_valgrind
        self.skip_perf_record = skip_perf_record
        self.outdir = outdir
        self.perf_record_callstack = perf_record_callstack
        self.perf_stat_num_repeats = perf_stat_num_repeats
        self.opensim_cmd = opensim_cmd

        if outdir and not os.path.exists(outdir):
            log.debug("%s: does not exist: creating", outdir)
            os.mkdir(outdir)

        if int(perf_stat_num_repeats) < 2:
            raise RuntimeError("number of repeats specified is too small. must be at least 2")

        log.debug("creating scratch directory")
        for path in test_case.files:
            for entry in glob.glob(path):
                log.debug("copying %s to %s", entry, self.scratch_dir)
                try:
                    shutil.copytree(entry, os.path.join(self.scratch_dir, os.path.basename(entry)))
                except OSError as exc:
                    if exc.errno == errno.ENOTDIR:
                        shutil.copy(entry, self.scratch_dir)
                    else:
                        raise
        log.debug("created scratch directory")

def opensim_args(ctx, test_case):
    args = [ctx.opensim_cmd]
    for lib in test_case.libraries:
        args += ["-L", lib]
    args += ["run-tool", test_case.setup_file]

    return args

def run_valgrind(ctx, test_case):
    p = subprocess.run(
        args=[
            "valgrind",
            "--tool=callgrind",
            "--dump-instr=yes",
            "--cache-sim=yes",
            "--branch-sim=yes",
            "--",
            *opensim_args(ctx, test_case),
        ],
        cwd=ctx.scratch_dir)
    assert p.returncode == 0

    if ctx.outdir:
        for outfile in glob.glob(os.path.join(ctx.scratch_dir, 'callgrind*')):
            log.debug("writing valgrind file '%s' to '%s'", outfile, ctx.outdir)
            shutil.copy(outfile, ctx.outdir)

def run_perf(ctx, test_case):
    if ctx.perf_record_callstack:
        cg_arg = ["--call-graph", "dwarf"]
    else:
        cg_arg = []

    p = subprocess.run(
        args=[
            "perf",
            "record",
            "--freq",
            "8192",
            *cg_arg,
            "--",
            *opensim_args(ctx, test_case),
        ],
        cwd=ctx.scratch_dir)
    assert p.returncode == 0

    if ctx.outdir:
        src = os.path.join(ctx.scratch_dir, 'perf.data')
        dest = ctx.outdir
        log.debug("writing perf file '%s' to '%s'", src, dest)
        shutil.copy(src, dest)

def run_test_case(ctx, test_case):
    if not ctx.skip_valgrind:
        run_valgrind(ctx, test_case)
    if not ctx.skip_perf_record:
        run_perf(ctx, test_case)

# Stats collected from `perf` output
class Stats:
    def __init__(self, time_elapsed, time_elapsed_stderr, time_elapsed_pct_stderr):
        self.time_elapsed = time_elapsed
        self.time_elapsed_stderr = time_elapsed_stderr
        self.time_elapsed_pct_stderr = time_elapsed_pct_stderr

def __run_test_case(ctx, test_case):
    start_time = time.time()
    p = subprocess.run(
        args=opensim_args(ctx, test_case),
        cwd=ctx.scratch_dir,
        stdout=None if log.isEnabledFor(logging.DEBUG) else subprocess.DEVNULL,
        stderr=None if log.isEnabledFor(logging.DEBUG) else subprocess.DEVNULL)
    end_time = time.time()
    duration = end_time - start_time
    return (p.returncode, duration)

def stat_test_case(ctx, test_case):
    stat_path = os.path.join(ctx.scratch_dir, "perf.stats")
    durations = []
    for i in range(0, int(ctx.perf_stat_num_repeats)):
        returncode, dur = __run_test_case(ctx, test_case)
        durations += [dur]
    duration_avg = sum(durations)/len(durations)
    duration_stddev = sum(map(lambda dur: (dur - duration_avg)**2, durations))/(len(durations)-1)
    pct_err = duration_stddev/duration_avg

    print(f"{test_case.name}: {'%.6f' % duration_avg} +- {'%.6f' % duration_stddev} seconds time elapsed ( +- {'%.3f' % pct_err}% )")

    return Stats(
            time_elapsed=duration_avg,
            time_elapsed_stderr=duration_stddev,
            time_elapsed_pct_stderr=pct_err)

# Storage of a comparison between two stats
class StoredComparison:
    def __init__(self, test_case, stat_master, stat_branch):
        self.test_case = test_case
        self.stat_master = stat_master
        self.stat_branch = stat_branch

# Thin abstraction for collecting + printing output stats at runtime.
# This exists because I plan on having a `CliReporter` (interactive)
# and a `TableReporter` (collects inputs and aggregates into a nice
# table).
class Reporter:
    def __init__(self, num_repeats, lhs_opensim, rhs_opensim):
        print(f"lhs = {lhs_opensim}")
        print(f"rhs = {rhs_opensim}")
        print(f"repeats = {num_repeats}")
        self.comparisons = []

    def on_comparison(self, test_case, stat_master, stat_branch):
        sc = StoredComparison(test_case, stat_master, stat_branch)
        self.comparisons.append(sc)

    def on_finish(self):
        names = [sc.test_case for sc in self.comparisons]
        master_elapseds = [sc.stat_master.time_elapsed for sc in self.comparisons]
        master_stderrs = [sc.stat_master.time_elapsed_stderr for sc in self.comparisons]
        branch_elapseds = [sc.stat_branch.time_elapsed for sc in self.comparisons]
        branch_stderrs = [sc.stat_branch.time_elapsed_stderr for sc in self.comparisons]
        speedups = list(map(lambda p: p[0]/p[1], zip(master_elapseds, branch_elapseds)))

        to_2dp_str = lambda f: "%.2f" % f
        master_strs = [to_2dp_str(v) for v in master_elapseds]
        master_stderrs_strs = [to_2dp_str(v) for v in master_stderrs]
        branch_strs = [to_2dp_str(v) for v in branch_elapseds]
        branch_stderrs_strs = [to_2dp_str(v) for v in branch_stderrs]
        speedup_strs = [to_2dp_str(v) for v in speedups]

        cols = [
            names,
            master_strs,
            master_stderrs_strs,
            branch_strs,
            branch_stderrs_strs,
            speedup_strs,
        ]
        col_widths = [max(map(len, col)) for col in cols]

        headers = [
            "Test Name",
            "lhs [secs]",
            "σ [secs]",
            "rhs [secs]",
            "σ [secs]",
            "Speedup",
        ]

        # hack: fixup col widths for headers
        for i in range(0, len(col_widths)):
            col_widths[i] = max(col_widths[i], len(headers[i]))

        # print headers
        header_line = "|"
        for i in range(0, len(headers)):
            col_width = col_widths[i]
            header_line += ' '
            header_line += headers[i].rjust(col_width)
            header_line += ' |'
        print(header_line)

        # seperator
        separator_line = "|"
        for i in range(0, len(headers)):
            col_width = col_widths[i]
            separator_line += ' '
            separator_line += col_width * '-'
            separator_line += ' |'
        print(separator_line)

        # data rows
        for rownum in range(0, len(self.comparisons)):
            s = "|"
            for colnum in range(0, len(cols)):
                width = col_widths[colnum]
                data = cols[colnum][rownum]
                s += ' '
                s += data.rjust(width)
                s += ' |'
            print(s)

def get_testcase_by_name(st, test_name):
    for test_case in st.test_cases:
        if test_case.name == test_name:
            return test_case

    raise RuntimeError(f"{test_name}: no test case found: try 'ls' to list all available cases")


# command: ls: list all available test cases
def cmd_ls(st, args):
    for test_case in st.test_cases:
        print(test_case.name)

# command: record: record the performance profile of a test case
def cmd_record(st, args):
    test_case = get_testcase_by_name(st, args.test_name)
    with tempfile.TemporaryDirectory() as tmpdir:
        ctx = TestContext(scratch_dir=tmpdir,
                          test_case=test_case,
                          skip_valgrind=args.skip_valgrind,
                          skip_perf_record=args.skip_perf_record,
                          outdir=args.output,
                          perf_record_callstack=args.perf_record_callstack)
        run_test_case(ctx, test_case)

# command: stat: report top-level walltime measurement of a test case
def cmd_stat(st, args):
    test_case = get_testcase_by_name(st, args.test_name)
    with tempfile.TemporaryDirectory() as tmpdir:
        ctx = TestContext(scratch_dir=tmpdir,
                          test_case=test_case,
                          outdir=args.output,
                          perf_stat_num_repeats=args.repeats)
        stat_test_case(ctx, test_case)

# command: compare-all: compare two opensim executables for all test
#                       cases and emit a markdown comparison table
def cmd_comparison(st, args):
    repeats = int(args.repeats)
    lhs_opensim = args.opensim_cmd_lhs
    rhs_opensim = args.opensim_cmd_rhs

    reporter = Reporter(num_repeats=repeats,
                        lhs_opensim=lhs_opensim,
                        rhs_opensim=rhs_opensim)

    for test_case in st.test_cases:
        with tempfile.TemporaryDirectory() as tmpdir:
            ctx = TestContext(scratch_dir=tmpdir,
                              test_case=test_case,
                              perf_stat_num_repeats=repeats,
                              opensim_cmd=lhs_opensim)
            stats_master = stat_test_case(ctx, test_case)
        with tempfile.TemporaryDirectory() as tmpdir:
            ctx = TestContext(scratch_dir=tmpdir,
                              test_case=test_case,
                              perf_stat_num_repeats=repeats,
                              opensim_cmd=rhs_opensim)
            stats_branch = stat_test_case(ctx, test_case)
        reporter.on_comparison(test_case.name, stats_master, stats_branch)
    reporter.on_finish()

def __permute_analyses(exes, test_cases, num_repeats):
    for exe in exes:
        for tc in test_cases:
            for i in range(0, int(num_repeats)):
                yield (exe, tc.name, i)

# command: full-analysis-configure
#
# configure a full analysis experiment. Effectively, generates a
# runfile that can be fed into `full-analysis-run`. The reason this is
# a two-step process is so that `full-analysis-run` can be continued
# if interrupted (it doesn't hold the run file internally, so it can
# be saved)
def cmd_full_analysis_configure(st, args):
    analyses = list(__permute_analyses(args.executables, st.test_cases, args.repeats))
    random.shuffle(analyses)

    test_id = 0
    for exe, tc, repeat in analyses:
        print(f"{test_id}:{exe}:{tc}:{repeat}")
        test_id += 1

def cmd_full_analysis_run(st, args):
    # this is inefficient, but simple. Shoot me *after* you find that
    # reading everything into memory, or doing two passes, is causing
    # issues

    analyses_to_run = []
    with open(args.runfile, 'rt') as fd:
        for line in fd:
            test_id, exe, tc, repeat = line.strip().split(':')
            analyses_to_run.append((test_id, exe, tc, repeat))

    # validation check: if the output file exists, verify that all
    # data in it matches expectations and note the last complete
    # analysis
    last_complete_analysis = 0
    if os.path.exists(args.outfile):
        with open(args.outfile, 'rt') as fd:
            for line in fd:
                expected = analyses_to_run[last_complete_analysis]
                test_id, exe, tc, repeat, _ = line.strip().split(':')
                got = (test_id, exe, tc, repeat)

                if expected != got:
                    raise RuntimeError(f"{args.outfile}: existing entry in output file does not match the input file: are you trying to continue an analysis with the wrong input file? {expected} {got}")

                last_complete_analysis += 1

    # all input analyses read and the resumption point (if any) has
    # been established: time to record output
    with open(args.outfile, 'at') as fd:
        for tc in analyses_to_run[last_complete_analysis:]:
            test_id, exe, tc, repeat = tc
            test_case = get_testcase_by_name(st, tc)
            with tempfile.TemporaryDirectory() as tmpdir:
                ctx = TestContext(scratch_dir=tmpdir,
                                  test_case=test_case,
                                  perf_stat_num_repeats=2,
                                  opensim_cmd=exe)
                returncode, dur = __run_test_case(ctx, test_case)
            if returncode != 0:
                output = f"{test_id}:{exe}:{tc}:{repeat}:FAIL"
            else:
                output = f"{test_id}:{exe}:{tc}:{repeat}:{dur}"
            log.debug(f"RESULT: {output}")
            fd.write(f"{output}\n")
            fd.flush()

def main():
    parser = argparse.ArgumentParser(
        description="OpenSim perf benchmark runner")
    parser.add_argument("-v", "--verbose", help="Enable verbose output", action="store_true")
    parser.add_argument("-t", "--tests", help="Search for tests in this dir (default: current dir)", default=os.curdir)
    subparsers = parser.add_subparsers(title="subcommands")

    # record
    if True:
        run_sp = subparsers.add_parser("record", help="Record detailed perf trace of a test")
        run_sp.add_argument(
            'test_name'
        )
        run_sp.add_argument(
            '-o',
            '--output',
            help="Output directory for recorded traces (default: temporary directory)",
            default=None
        )
        run_sp.add_argument(
            '--skip-valgrind',
            help="Do not record valgrind trace",
            action='store_true'
        )
        run_sp.add_argument(
            '--skip-perf-record',
            help="Do not record with 'perf record'",
            action='store_true'
        )
        run_sp.add_argument(
            '--perf-record-callstack',
            help="Record callstack when running perf",
            action='store_true'
        )
        run_sp.set_defaults(func=cmd_record)

    # ls
    if True:
        ls_sp = subparsers.add_parser("ls", help="List available tests")
        ls_sp.set_defaults(func=cmd_ls)

    # stat
    if True:
        measure_sp = subparsers.add_parser("stat", help="Measure top-level stats of a test")
        measure_sp.add_argument(
            'test_name'
        )
        measure_sp.add_argument(
            '-o',
            '--output',
            help="Output directory for recorded traces (default: temporary directory)",
            default=None
        )
        measure_sp.add_argument(
            '-r',
            '--repeats',
            help="Repeats per perf measurement",
            default=8
        )
        measure_sp.set_defaults(func=cmd_stat)

    # compare-all
    if True:
        comparison_sp = subparsers.add_parser("compare-all", help="compare two versions of opensim-cmd (defined using environment vars OPENSIM_MASTER_INSTALL and OPENSIM_BRANCH_INSTALL")
        comparison_sp.add_argument(
            'opensim_cmd_lhs',
            help='opensim-cmd command to run (e.g. opensim-cmd, /some/path/to/opensim-cmd)'
        )
        comparison_sp.add_argument(
            'opensim_cmd_rhs',
            help='opensim-cmd command to run (e.g. opensim-cmd, /some/other/path/to/opensim-cmd)'
        )
        comparison_sp.add_argument(
            '-r',
            '--repeats',
            help="Repeats per comparison",
            default=8
        )
        comparison_sp.set_defaults(func=cmd_comparison)

    # full-analysis-configure
    if True:
        fac_sp = subparsers.add_parser("full-analysis-configure", help="generate a randomized set of stat measurements to run on the provided executable")
        fac_sp.add_argument(
            'executables',
            help='full paths to opensim-cmd executables that should be tested',
            nargs='+')
        fac_sp.add_argument(
            '-r',
            '--repeats',
            help='Repeats per comparison',
            default=8)
        fac_sp.set_defaults(func=cmd_full_analysis_configure)

    if True:
        far_sp = subparsers.add_parser(
            "full-analysis-run",
            help="run a full analysis file generated by full-analysis-configure"
        )
        far_sp.add_argument(
            'runfile',
            help='runfile, as generated by full-analysis-configure'
        )
        far_sp.add_argument(
            'outfile',
            help='output result file: note, this process currently **continues** if the file already exists, rather than overwriting'
        )
        far_sp.set_defaults(func=cmd_full_analysis_run)

    # parse args
    args = parser.parse_args()

    # configure logging (should be done *before* anything else)
    if args.verbose:
        log.setLevel(logging.DEBUG)

    st = AppState(test_cases=__load_test_cases(args.tests))

    if hasattr(args, "func"):
        args.func(st, args)
        return 0
    else:
        parser.print_help(sys.stderr)
        return -1

if __name__ == "__main__":
    main()
